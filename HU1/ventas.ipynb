{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cef24cd",
   "metadata": {},
   "source": [
    "# Complete ETL: Cleaning, Normalization and Loading into PostgreSQL\n",
    "**Sales Dataset — 1,048,575 records**\n",
    "\n",
    "This notebook contains the full workflow:\n",
    "\n",
    "1. Initial exploration\n",
    "2. Dataset cleaning\n",
    "3. Field normalization\n",
    "4. Creation of dimension tables and fact table\n",
    "5. Export to CSV\n",
    "6. Load into PostgreSQL using SQLAlchemy + Environment Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0373e35",
   "metadata": {},
   "source": [
    "## 1. Importing libraries and loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "748bbe66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Ciudad       Fecha Producto       Tipo_Producto Cantidad  \\\n",
      "0                Santiago  2025-10-30    Arepa           Abarrotes      2.0   \n",
      "1                 Córdoba  2025-11-17    Arepa           Abarrotes      7.0   \n",
      "2            Barranquilla  2025-10-22    Leche              Lácteo      9.0   \n",
      "3                New York  2025-10-20   Cereal              Lácteo      3.0   \n",
      "4                  Madrid  2025-10-20    Leche               Hogar      2.0   \n",
      "...                   ...         ...      ...                 ...      ...   \n",
      "1048570       Guadalajara  2025-10-23   Yogurt              Lácteo      9.0   \n",
      "1048571  Ciudad de México  2025-11-13  Gaseosa               Hogar      7.0   \n",
      "1048572              Lima  2025-10-30    Arepa              Bebida      8.0   \n",
      "1048573            Madrid  2025-10-23     Café           Abarrotes     10.0   \n",
      "1048574             Cusco  2025-10-22    Queso  Alimento_Percedero      1.0   \n",
      "\n",
      "        Precio_Unitario  Tipo_de_Venta Tipo_Cliente Descuento Costo_Envio  \\\n",
      "0                3681.0         Online    Minorista       0.2         0.0   \n",
      "1                2321.0   Distribuidor     Gobierno      0.15         0.0   \n",
      "2                3540.0   Distribuidor     Gobierno       0.2         0.0   \n",
      "3                3287.0  Tienda_Física     Gobierno      0.05         0.0   \n",
      "4                3414.0   Distribuidor    Mayorista       0.0         0.0   \n",
      "...                 ...            ...          ...       ...         ...   \n",
      "1048570          4325.0         Online    Mayorista      0.05         0.0   \n",
      "1048571          1187.0  Tienda_Física     Gobierno      0.15         0.0   \n",
      "1048572          3575.0            NaN  Corporativo       0.1         0.0   \n",
      "1048573          2073.0         Online  Corporativo       0.0         0.0   \n",
      "1048574          1192.0    Call_Center    Minorista       0.0         0.0   \n",
      "\n",
      "        Total_Venta  \n",
      "0            5889.0  \n",
      "1           13809.0  \n",
      "2           25488.0  \n",
      "3            9367.0  \n",
      "4            6828.0  \n",
      "...             ...  \n",
      "1048570     36978.0  \n",
      "1048571      7062.0  \n",
      "1048572     25740.0  \n",
      "1048573     20730.0  \n",
      "1048574      1192.0  \n",
      "\n",
      "[1048575 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import unicodedata\n",
    "import re\n",
    "import os\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "df = pd.read_csv(\"../ventas.csv\")\n",
    "\n",
    "print(df)\n",
    "# [1.048.575 rows x 11 columns]\n",
    "# 11 Columnas\n",
    "# 1.048.575 Registros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d717d0b",
   "metadata": {},
   "source": [
    "## 2. Initial data inspection\n",
    "View unique values per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f76d330d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Ciudad' ['Santiago' 'Córdoba' 'Barranquilla' 'New York' 'Madrid' 'Pereira'\n",
      " 'Trujillo' 'Valencia' 'Puebla' 'Mendoza' 'Lima' 'Barcelona' 'Monterrey'\n",
      " 'Ciudad de México' 'Concepción' 'Antofagasta' 'Rosario' 'Guadalajara'\n",
      " 'Chicago' 'Bucaramanga' 'Cali' 'Cartagena' 'Medellín' 'Cusco' 'Houston'\n",
      " 'Los Angeles' 'Arequipa' 'Valparaíso' 'Buenos Aires' 'Miami' 'Sevilla'\n",
      " 'Tijuana' 'Bogotá' '  Arequipa   ' nan '  Antofagasta   ' 'Pereira@@@'\n",
      " 'Buenos Aires###' 'mONTERREY' 'Cusco***' '  Guadalajara   '\n",
      " '  Córdoba   ' 'mENDOZA' 'vALPARAÍSO' '  Trujillo   ' 'lOS aNGELES'\n",
      " 'cALI' 'Houston***' '  Chicago   ' 'Miami***' 'aNTOFAGASTA' 'mEDELLÍN'\n",
      " 'Monterrey@@@' 'Madrid***' 'Tijuana###' '  Valencia   ' 'gUADALAJARA'\n",
      " 'Miami@@@' 'vALENCIA' 'pEREIRA' 'cÓRDOBA' 'Córdoba###' 'bUCARAMANGA'\n",
      " 'New York***' '  Concepción   ' 'bARRANQUILLA' 'rOSARIO' '  Barcelona   '\n",
      " 'pUEBLA' 'cUSCO' 'Antofagasta###' '  Miami   ' 'lIMA' '  Pereira   '\n",
      " '  Lima   ' '  Mendoza   ' '  Los Angeles   ' 'tIJUANA' 'Arequipa@@@'\n",
      " 'aREQUIPA' 'cIUDAD DE mÉXICO' '  Valparaíso   ' 'bOGOTÁ' 'sEVILLA'\n",
      " 'Monterrey***' '  Madrid   ' 'Chicago@@@' 'Trujillo***' 'bARCELONA'\n",
      " 'Barcelona@@@' 'Cartagena***' '  New York   ' 'mADRID' 'Barcelona***'\n",
      " 'Valparaíso###' 'Puebla***' '  Tijuana   ' '  Santiago   ' 'Trujillo###'\n",
      " '  Sevilla   ' 'hOUSTON' 'Lima@@@' 'Guadalajara***' 'Tijuana***'\n",
      " '  Buenos Aires   ' 'Concepción###' '  Medellín   ' 'Bogotá@@@'\n",
      " 'Buenos Aires***' 'Arequipa***' 'Barcelona###' 'New York###'\n",
      " 'Valparaíso@@@' 'Arequipa###' 'Trujillo@@@' 'bUENOS aIRES' 'Cusco@@@'\n",
      " 'Santiago***' 'Sevilla###' 'Córdoba@@@' 'mIAMI' 'cHICAGO' 'sANTIAGO'\n",
      " '  Bogotá   ' '  Houston   ' 'Sevilla***' 'Mendoza###' '  Cartagena   '\n",
      " 'Bucaramanga***' '  Rosario   ' 'Córdoba***' 'cONCEPCIÓN'\n",
      " 'Buenos Aires@@@' '  Cusco   ' 'tRUJILLO' 'Santiago###' 'Medellín@@@'\n",
      " 'Valencia@@@' 'Los Angeles###' 'Puebla@@@' '  Ciudad de México   '\n",
      " 'Puebla###' 'Rosario***' 'Concepción@@@' 'Houston@@@' 'Bucaramanga@@@'\n",
      " 'Santiago@@@' 'Lima***' 'Mendoza@@@' '  Monterrey   ' 'Monterrey###'\n",
      " '  Puebla   ' 'Cusco###' 'Tijuana@@@' '  Cali   ' 'Bogotá***'\n",
      " 'Antofagasta@@@' 'Lima###' 'Madrid###' 'nEW yORK' 'Sevilla@@@'\n",
      " 'Los Angeles@@@' 'Guadalajara@@@' 'Bogotá###' 'Chicago***'\n",
      " '  Bucaramanga   ' 'Rosario###' 'Madrid@@@' 'Concepción***'\n",
      " 'Ciudad de México***' 'Houston###' 'Barranquilla###' 'cARTAGENA'\n",
      " 'Bucaramanga###' 'Guadalajara###' '  Barranquilla   ' 'Valencia***'\n",
      " 'New York@@@' 'Valparaíso***' 'Medellín***' 'Los Angeles***' 'Cali@@@'\n",
      " 'Ciudad de México@@@' 'Cartagena@@@' 'Ciudad de México###' 'Rosario@@@']\n",
      "\n",
      "'Fecha' ['2025-10-30' '2025-11-17' '2025-10-22' '2025-10-20' '2025-11-13'\n",
      " '2025-11-03' '2025-11-15' '2025-10-21' '2025-10-24' '2025-10-25'\n",
      " '2025-10-28' '2025-10-27' '2025-11-14' '2025-11-01' '2025-11-07'\n",
      " '2025-11-16' '2025-10-29' '2025-10-26' '2025-11-05' '2025-11-18'\n",
      " '2025-10-23' '2025-11-09' '2025-11-04' '2025-11-10' '2025-11-12'\n",
      " '2025-11-08' '2025-11-06' '2025-11-11' '2025-11-02' '2025-10-31'\n",
      " '2025-10-22###' '2025-10-23@@@' nan '2025-10-25###' '2025-11-05***'\n",
      " '2025-11-16@@@' '2025-11-03###' '2025-10-24***' '2025-10-20###'\n",
      " '2025-11-17###' '2025-11-08@@@' '2025-11-07***' '2025-11-07@@@'\n",
      " '2025-11-17***' '2025-11-06###' '2025-10-28***' '2025-11-12###'\n",
      " '2025-11-16###' '2025-10-31@@@' '2025-11-10###' '2025-11-12***'\n",
      " '2025-11-15***' '2025-11-12@@@' '2025-11-13***' '2025-10-30###'\n",
      " '2025-10-20@@@' '2025-10-21***' '2025-11-15###' '2025-11-09###'\n",
      " '2025-10-27@@@' '2025-10-23###' '2025-11-06@@@' '2025-11-05@@@'\n",
      " '2025-11-02@@@' '2025-10-28###' '2025-10-26***' '2025-11-16***'\n",
      " '2025-11-18###' '2025-11-13###' '2025-11-10@@@' '2025-11-09@@@'\n",
      " '2025-11-10***' '2025-10-24@@@' '2025-11-14###' '2025-11-11***'\n",
      " '2025-10-30@@@' '2025-10-20***' '2025-11-11@@@' '2025-11-09***'\n",
      " '2025-10-25***' '2025-11-07###' '2025-11-01@@@' '2025-10-21###'\n",
      " '2025-11-18***' '2025-11-06***' '2025-11-04***' '2025-11-02***'\n",
      " '2025-10-22@@@' '2025-11-01###' '2025-10-21@@@' '2025-11-01***'\n",
      " '2025-11-15@@@' '2025-10-27###' '2025-10-30***' '2025-11-02###'\n",
      " '2025-11-14***' '2025-11-11###' '2025-11-14@@@' '2025-11-04###'\n",
      " '2025-10-28@@@' '2025-10-22***' '2025-11-03@@@' '2025-10-26@@@'\n",
      " '2025-10-31###' '2025-10-23***' '2025-11-08###' '2025-11-04@@@'\n",
      " '2025-11-08***' '2025-10-26###' '2025-11-05###' '2025-10-29###'\n",
      " '2025-11-17@@@']\n",
      "\n",
      "'Producto' ['Arepa' 'Leche' 'Cereal' 'Queso' 'Chocolate' 'Té' 'Pan' 'Mantequilla'\n",
      " 'Gaseosa' 'Yogurt' 'Galletas' 'Café' nan '  Chocolate   ' '  Galletas   '\n",
      " 'qUESO' 'Mantequilla###' '  Queso   ' 'Galletas@@@' 'cAFÉ' 'yOGURT'\n",
      " '  Cereal   ' 'Arepa***' 'gALLETAS' 'aREPA' 'Cereal@@@' 'Pan@@@'\n",
      " '  Café   ' 'Té@@@' 'Mantequilla@@@' 'Yogurt@@@' 'Té###' 'mANTEQUILLA'\n",
      " 'Cereal###' 'Café@@@' 'tÉ' 'Leche###' 'Chocolate***' 'Té***' '  Leche   '\n",
      " 'gASEOSA' 'pAN' 'lECHE' 'Galletas***' '  Yogurt   ' 'Yogurt***' '  Té   '\n",
      " 'Galletas###' 'Cereal***' '  Mantequilla   ' '  Pan   ' 'Chocolate###'\n",
      " 'Arepa@@@' '  Gaseosa   ' 'cEREAL' 'Pan###' 'Café***' 'Leche***'\n",
      " 'Yogurt###' '  Arepa   ' 'cHOCOLATE' 'Gaseosa***' 'Pan***' 'Queso***'\n",
      " 'Café###' 'Arepa###' 'Chocolate@@@' 'Gaseosa@@@' 'Queso###' 'Gaseosa###'\n",
      " 'Leche@@@' 'Mantequilla***' 'Queso@@@']\n",
      "\n",
      "'Tipo_Producto' ['Abarrotes' 'Lácteo' 'Hogar' 'Bebida' 'Snack' 'Alimento_Percedero'\n",
      " 'Alimento_Percedero***' nan 'Abarrotes###' '  Snack   ' 'Abarrotes@@@'\n",
      " 'aBARROTES' 'bEBIDA' '  Bebida   ' '  Alimento_Percedero   ' 'Bebida###'\n",
      " '  Hogar   ' 'sNACK' 'Hogar***' '  Abarrotes   ' 'Lácteo@@@' 'Bebida***'\n",
      " 'hOGAR' '  Lácteo   ' 'Alimento_Percedero@@@' 'aLIMENTO_pERCEDERO'\n",
      " 'Lácteo###' 'Snack###' 'Abarrotes***' 'lÁCTEO' 'Hogar###'\n",
      " 'Alimento_Percedero###' 'Snack@@@' 'Snack***' 'Bebida@@@' 'Lácteo***'\n",
      " 'Hogar@@@']\n",
      "\n",
      "'Cantidad' ['2.0' '7.0' '9.0' '3.0' '5.0' '8.0' '6.0' '4.0' '10.0' '1.0' nan '???'\n",
      " '300.0' '250.0' '350.0' '200.0' '20.0' '50.0' '400.0' '225.0' '100.0'\n",
      " '175.0' '75.0' '450.0' '7500.0' '125.0' '60.0' '500.0' '80.0' '40.0'\n",
      " '150.0' '70.0' '90.0' '30.0' '25.0']\n",
      "\n",
      "'Precio_Unitario' ['3681.0' '2321.0' '3540.0' ... '49350.0' '45250.0' '120100.0']\n",
      "\n",
      "'Tipo_de_Venta' ['Online' 'Distribuidor' 'Tienda_Física' 'Call_Center' 'Tienda_Física###'\n",
      " 'Online@@@' 'Call_Center###' nan '  Tienda_Física   ' 'oNLINE'\n",
      " '  Online   ' '  Call_Center   ' 'Distribuidor###' 'Online###'\n",
      " 'cALL_cENTER' '  Distribuidor   ' 'dISTRIBUIDOR' 'Distribuidor@@@'\n",
      " 'tIENDA_fÍSICA' 'Tienda_Física***' 'Call_Center***' 'Tienda_Física@@@'\n",
      " 'Online***' 'Call_Center@@@' 'Distribuidor***']\n",
      "\n",
      "'Tipo_Cliente' ['Minorista' 'Gobierno' 'Mayorista' 'Corporativo' nan '  Gobierno   '\n",
      " 'cORPORATIVO' '  Mayorista   ' 'gOBIERNO' 'Mayorista@@@' 'Corporativo@@@'\n",
      " '  Corporativo   ' 'mAYORISTA' 'Corporativo###' '  Minorista   '\n",
      " 'Mayorista***' 'Gobierno@@@' 'Gobierno***' 'mINORISTA' 'Mayorista###'\n",
      " 'Minorista@@@' 'Minorista###' 'Minorista***' 'Corporativo***'\n",
      " 'Gobierno###']\n",
      "\n",
      "'Descuento' ['0.2' '0.15' '0.05' '0.0' '0.1' '???' nan]\n",
      "\n",
      "'Costo_Envio' ['0.0' '10000.0' '5000.0' '???' '100000.0' nan '50000.0' '500000.0'\n",
      " '250000.0' '125000.0']\n",
      "\n",
      "'Total_Venta' ['5889.0' '13809.0' '25488.0' ... '737350.0' '41646.0' '52075.0']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row in df.columns:\n",
    "    print(f\"'{row}' {df[row].unique()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef50b954",
   "metadata": {},
   "source": [
    "Preview of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2f951351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Ciudad       Fecha Producto Tipo_Producto Cantidad Precio_Unitario  \\\n",
      "0      Santiago  2025-10-30    Arepa     Abarrotes      2.0          3681.0   \n",
      "1       Córdoba  2025-11-17    Arepa     Abarrotes      7.0          2321.0   \n",
      "2  Barranquilla  2025-10-22    Leche        Lácteo      9.0          3540.0   \n",
      "3      New York  2025-10-20   Cereal        Lácteo      3.0          3287.0   \n",
      "4        Madrid  2025-10-20    Leche         Hogar      2.0          3414.0   \n",
      "\n",
      "   Tipo_de_Venta Tipo_Cliente Descuento Costo_Envio Total_Venta  \n",
      "0         Online    Minorista       0.2         0.0      5889.0  \n",
      "1   Distribuidor     Gobierno      0.15         0.0     13809.0  \n",
      "2   Distribuidor     Gobierno       0.2         0.0     25488.0  \n",
      "3  Tienda_Física     Gobierno      0.05         0.0      9367.0  \n",
      "4   Distribuidor    Mayorista       0.0         0.0      6828.0  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c3ba0e",
   "metadata": {},
   "source": [
    "Dataset info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6f85bebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1048575 entries, 0 to 1048574\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count    Dtype \n",
      "---  ------           --------------    ----- \n",
      " 0   Ciudad           1047607 non-null  object\n",
      " 1   Fecha            1047629 non-null  object\n",
      " 2   Producto         1047632 non-null  object\n",
      " 3   Tipo_Producto    1047660 non-null  object\n",
      " 4   Cantidad         1047611 non-null  object\n",
      " 5   Precio_Unitario  1047618 non-null  object\n",
      " 6   Tipo_de_Venta    1047585 non-null  object\n",
      " 7   Tipo_Cliente     1047627 non-null  object\n",
      " 8   Descuento        1047657 non-null  object\n",
      " 9   Costo_Envio      1047675 non-null  object\n",
      " 10  Total_Venta      1047592 non-null  object\n",
      "dtypes: object(11)\n",
      "memory usage: 88.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ad8b93",
   "metadata": {},
   "source": [
    "Count missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "eb998aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ciudad             968\n",
      "Fecha              946\n",
      "Producto           943\n",
      "Tipo_Producto      915\n",
      "Cantidad           964\n",
      "Precio_Unitario    957\n",
      "Tipo_de_Venta      990\n",
      "Tipo_Cliente       948\n",
      "Descuento          918\n",
      "Costo_Envio        900\n",
      "Total_Venta        983\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffbb0a2",
   "metadata": {},
   "source": [
    "statistics values (Descriptive statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "08b56dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Ciudad       Fecha Producto Tipo_Producto Cantidad Precio_Unitario  \\\n",
      "count    1047607     1047629  1047632       1047660  1047611         1047618   \n",
      "unique       185         111       72            36       34            5013   \n",
      "top     Trujillo  2025-10-31     Café        Bebida     10.0             ???   \n",
      "freq       37703       35279    87569        174793   104992             729   \n",
      "\n",
      "       Tipo_de_Venta Tipo_Cliente Descuento Costo_Envio Total_Venta  \n",
      "count        1047585      1047627   1047657     1047675     1047592  \n",
      "unique            24           24         6           9       49360  \n",
      "top     Distribuidor     Gobierno      0.15         0.0         ???  \n",
      "freq          262457       262228    210077      697490         691  \n"
     ]
    }
   ],
   "source": [
    "print(df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578ee5f2",
   "metadata": {},
   "source": [
    "## 3. Cleaning and normalizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "608a278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../ventas.csv\")\n",
    "def limpiar_ciudad(texto):\n",
    "    if pd.isna(texto):\n",
    "        return texto\n",
    "    \n",
    "    # Converte el texto a minúsculas\n",
    "    texto = texto.lower().strip()\n",
    "\n",
    "    # Elimina los acentos\n",
    "    texto = ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', texto)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "    # Elimina caracteres especiales (dejando solo las letras)\n",
    "    texto = re.sub(r'[^a-z\\s]', '', texto)\n",
    "\n",
    "    # Quita los espacios dobles\n",
    "    texto = re.sub(r'\\s+', ' ', texto)\n",
    "\n",
    "    return texto\n",
    "# 1. Normalización de texto\n",
    "cols_texto = [\"Ciudad\", \"Producto\", \"Tipo_Producto\", \"Tipo_de_Venta\", \"Tipo_Cliente\"]\n",
    "for col in cols_texto:\n",
    "    df[col] = df[col].apply(limpiar_ciudad)\n",
    "\n",
    "# 2. Converción a numéricos\n",
    "df[\"Descuento\"] = pd.to_numeric(df[\"Descuento\"], errors=\"coerce\")\n",
    "df[\"Precio_Unitario\"] = pd.to_numeric(df[\"Precio_Unitario\"], errors=\"coerce\")\n",
    "df[\"Cantidad\"] = pd.to_numeric(df[\"Cantidad\"], errors=\"coerce\")\n",
    "df[\"Costo_Envio\"] = pd.to_numeric(df[\"Costo_Envio\"], errors=\"coerce\")\n",
    "df[\"Total_Venta\"] = pd.to_numeric(df[\"Total_Venta\"], errors=\"coerce\")\n",
    "\n",
    "# 3. Rellenar faltantes\n",
    "df[\"Tipo_Cliente\"] = df[\"Tipo_Cliente\"].replace(\"nan\", np.nan)\n",
    "df[\"Tipo_Cliente\"] = df[\"Tipo_Cliente\"].fillna(\"desconocido\")\n",
    "\n",
    "df[\"Descuento\"] = df[\"Descuento\"].fillna(0)\n",
    "df[\"Precio_Unitario\"] = df[\"Precio_Unitario\"].fillna(0)\n",
    "df[\"Cantidad\"] = df[\"Cantidad\"].fillna(0)\n",
    "df[\"Costo_Envio\"] = df[\"Costo_Envio\"].fillna(0)\n",
    "df[\"Total_Venta\"] = df[\"Total_Venta\"].fillna(0)\n",
    "\n",
    "# 4. Converción a fecha\n",
    "df[\"Fecha\"] = pd.to_datetime(df[\"Fecha\"], errors=\"coerce\")\n",
    "\n",
    "\n",
    "# 5. Recalcular el valor de Total_Venta\n",
    "df[\"Total_Venta\"] = (df[\"Cantidad\"] * df[\"Precio_Unitario\"]) - (df[\"Descuento\"] * df[\"Cantidad\"] * df[\"Precio_Unitario\"]) + df[\"Costo_Envio\"]\n",
    "\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "# 6. Guarda los cambios en  un nuevo archivo .csv\n",
    "\n",
    "df.to_csv(\"../ventas_limpias.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "83e05f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   ciudad      fecha producto      tipo_producto  cantidad  \\\n",
      "0                santiago 2025-10-30    arepa          abarrotes       2.0   \n",
      "1                 cordoba 2025-11-17    arepa          abarrotes       7.0   \n",
      "2            barranquilla 2025-10-22    leche             lacteo       9.0   \n",
      "3                new york 2025-10-20   cereal             lacteo       3.0   \n",
      "4                  madrid 2025-10-20    leche              hogar       2.0   \n",
      "...                   ...        ...      ...                ...       ...   \n",
      "1048570       guadalajara 2025-10-23   yogurt             lacteo       9.0   \n",
      "1048571  ciudad de mexico 2025-11-13  gaseosa              hogar       7.0   \n",
      "1048572              lima 2025-10-30    arepa             bebida       8.0   \n",
      "1048573            madrid 2025-10-23     cafe          abarrotes      10.0   \n",
      "1048574             cusco 2025-10-22    queso  alimentopercedero       1.0   \n",
      "\n",
      "         precio_unitario tipo_de_venta tipo_cliente  descuento  costo_envio  \\\n",
      "0                 3681.0        online    minorista       0.20          0.0   \n",
      "1                 2321.0  distribuidor     gobierno       0.15          0.0   \n",
      "2                 3540.0  distribuidor     gobierno       0.20          0.0   \n",
      "3                 3287.0  tiendafisica     gobierno       0.05          0.0   \n",
      "4                 3414.0  distribuidor    mayorista       0.00          0.0   \n",
      "...                  ...           ...          ...        ...          ...   \n",
      "1048570           4325.0        online    mayorista       0.05          0.0   \n",
      "1048571           1187.0  tiendafisica     gobierno       0.15          0.0   \n",
      "1048572           3575.0           NaN  corporativo       0.10          0.0   \n",
      "1048573           2073.0        online  corporativo       0.00          0.0   \n",
      "1048574           1192.0    callcenter    minorista       0.00          0.0   \n",
      "\n",
      "         total_venta  \n",
      "0            5889.60  \n",
      "1           13809.95  \n",
      "2           25488.00  \n",
      "3            9367.95  \n",
      "4            6828.00  \n",
      "...              ...  \n",
      "1048570     36978.75  \n",
      "1048571      7062.65  \n",
      "1048572     25740.00  \n",
      "1048573     20730.00  \n",
      "1048574      1192.00  \n",
      "\n",
      "[1048575 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768dc88a",
   "metadata": {},
   "source": [
    "## 4. PostgreSQL connection configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f42b307e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection Succesfull!!\n"
     ]
    }
   ],
   "source": [
    "#Carga el archivo .env\n",
    "load_dotenv(override=True)\n",
    "\n",
    "#Variables de entorno\n",
    "\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\")\n",
    "\n",
    "\n",
    "URL = f\"postgresql+psycopg2://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "engine = create_engine(URL)\n",
    "\n",
    "conn = engine.connect()\n",
    "try:\n",
    "    print(\"Connection Succesfull!!\" if conn else \"\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error al conectar la base datos en\", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5122a7ed",
   "metadata": {},
   "source": [
    "## 5. Schema verification in PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3c34d9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Talbe: 'Ciudad' Exist and is Filled.\n",
      "Talbe: 'Tipo_producto' Exist and is Filled.\n",
      "Talbe: 'Producto' Exist and is Filled.\n",
      "Talbe: 'Tipo_venta' Exist and is Filled.\n",
      "Talbe: 'Tipo_cliente' Exist and is Filled.\n",
      "Talbe: 'Factura_ventas' Exist and is Filled.\n"
     ]
    }
   ],
   "source": [
    "# Setting schema\n",
    "\n",
    "set_schema = conn.execute(text(\"set search_path to riwi_ventas;\"))\n",
    "\n",
    "# Array to verify the status of each one table\n",
    "tables = [\"ciudad\", \"tipo_producto\", \"producto\", \"tipo_venta\", \"tipo_cliente\", \"factura_ventas\"]\n",
    "\n",
    "# This loop verfy the status\n",
    "for t in range(len(tables)):\n",
    "    verify = pd.read_sql((f\"select * from {tables[t]};\"),conn)\n",
    "    if not verify.empty:\n",
    "        print(f\"Talbe: '{tables[t].capitalize()}' Exist and is Filled.\")\n",
    "    if verify.empty:\n",
    "        print(f\"Table: '{tables[t].capitalize()}' doesn't Exist or is Empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a4aab2",
   "metadata": {},
   "source": [
    "## 6. Create dimension tables and fact table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "de607850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV creados correctamente en la carpeta 'tables_csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "cleaned_df = pd.read_csv(\"../ventas_limpias.csv\")\n",
    "\n",
    "#Obtain the unique cities and generate the ID for Cities\n",
    "ciudad_df = (\n",
    "    cleaned_df[\"ciudad\"].drop_duplicates()\n",
    "                .reset_index(drop=True)\n",
    "                .reset_index()\n",
    "                .rename(columns={\"index\": \"ciudad_id\", \"ciudad\": \"nombre_ciudad\"})\n",
    "                \n",
    ")\n",
    "ciudad_df[\"ciudad_id\"] += 1\n",
    "\n",
    "cleaned_df =cleaned_df.merge(ciudad_df, left_on=\"ciudad\", right_on=\"nombre_ciudad\", how=\"left\",suffixes=(\"\", \"_drop\"))\n",
    "\n",
    "\n",
    "# Obtain the unique cities and generate the ID for Products\n",
    "\n",
    "tipo_producto_df = (\n",
    "    cleaned_df[\"tipo_producto\"].drop_duplicates()\n",
    "                       .reset_index(drop=True)\n",
    "                       .reset_index()\n",
    "                       .rename(columns={\"index\": \"tipo_producto_id\"})\n",
    ")\n",
    "tipo_producto_df[\"tipo_producto_id\"] += 1\n",
    "\n",
    "cleaned_df =cleaned_df.merge(tipo_producto_df, on=\"tipo_producto\", how=\"left\",suffixes=(\"\", \"_drop\"))\n",
    "\n",
    "\n",
    "# Obtain the unique cities and generate the ID for type products\n",
    "\n",
    "cleaned_df = cleaned_df.rename(columns={\"producto\":\"nombre_producto\"}) #Rename to prevent conflicts with DB\n",
    "\n",
    "producto_df = (\n",
    "    cleaned_df[[\"nombre_producto\", \"tipo_producto_id\"]]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"producto_id\", \"producto\":\"nombre_producto\"})\n",
    ")\n",
    "producto_df[\"producto_id\"] += 1\n",
    "\n",
    "cleaned_df =cleaned_df.merge(producto_df, on=[\"nombre_producto\", \"tipo_producto_id\"], how=\"left\",suffixes=(\"\", \"_drop\"))\n",
    "\n",
    "# Obtain the unique cities and generate the ID for type sale\n",
    "\n",
    "cleaned_df = cleaned_df.rename(columns={\"tipo_de_venta\":\"tipo_venta\"}) #Rename to prevent conflicts with DB\n",
    "\n",
    "tipo_venta_df = (\n",
    "    cleaned_df[\"tipo_venta\"].drop_duplicates()\n",
    "                       .reset_index(drop=True)\n",
    "                       .reset_index()\n",
    "                       .rename(columns={\"index\": \"tipo_venta_id\"})\n",
    ")\n",
    "tipo_venta_df[\"tipo_venta_id\"] += 1\n",
    "\n",
    "cleaned_df =cleaned_df.merge(tipo_venta_df, on=\"tipo_venta\", how=\"left\",suffixes=(\"\", \"_drop\"))\n",
    "\n",
    "\n",
    "# Obtain the unique cities and generate the ID for type customer\n",
    "tipo_cliente_df = (\n",
    "    cleaned_df[\"tipo_cliente\"].drop_duplicates()\n",
    "                      .reset_index(drop=True)\n",
    "                      .reset_index()\n",
    "                      .rename(columns={\"index\": \"tipo_cliente_id\"})\n",
    ")\n",
    "tipo_cliente_df[\"tipo_cliente_id\"] += 1\n",
    "\n",
    "cleaned_df =cleaned_df.merge(tipo_cliente_df, on=\"tipo_cliente\", how=\"left\",suffixes=(\"\", \"_drop\"))\n",
    "\n",
    "\n",
    "#Create the table for sales\n",
    "factura_df =cleaned_df[[\n",
    "    \"fecha\", \"ciudad_id\", \"producto_id\", \"tipo_venta_id\",\n",
    "    \"tipo_cliente_id\", \"cantidad\", \"precio_unitario\",\n",
    "    \"descuento\", \"costo_envio\", \"total_venta\"\n",
    "]].copy()\n",
    "\n",
    "\n",
    "#export tables to external folder\n",
    "os.makedirs(\"../tables_csv\", exist_ok=True)\n",
    "\n",
    "#Save the files into tables_csv\n",
    "ciudad_df.to_csv(\"../tables_csv/ciudad.csv\", index=False)\n",
    "tipo_producto_df.to_csv(\"../tables_csv/tipo_producto.csv\", index=False)\n",
    "producto_df.to_csv(\"../tables_csv/producto.csv\", index=False)\n",
    "tipo_venta_df.to_csv(\"../tables_csv/tipo_venta.csv\", index=False)\n",
    "tipo_cliente_df.to_csv(\"../tables_csv/tipo_cliente.csv\", index=False)\n",
    "factura_df.to_csv(\"../tables_csv/factura_ventas.csv\", index=False)\n",
    "\n",
    "print(\"CSV creados correctamente en la carpeta 'tables_csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623d0499",
   "metadata": {},
   "source": [
    "## 7. Export tables to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "15c2bd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Insert into table: ciudad\n",
      "  → Chunk 1 inserting... (34 rows)\n"
     ]
    },
    {
     "ename": "IntegrityError",
     "evalue": "(psycopg2.errors.UniqueViolation) llave duplicada viola restricción de unicidad «ciudad_pkey»\nDETAIL:  Ya existe la llave (ciudad_id)=(1).\n\n[SQL: \n                INSERT INTO ciudad (ciudad_id, nombre_ciudad)\n                VALUES (%(ciudad_id)s, %(nombre_ciudad)s);\n            ]\n[parameters: [{'ciudad_id': 1, 'nombre_ciudad': 'santiago'}, {'ciudad_id': 2, 'nombre_ciudad': 'cordoba'}, {'ciudad_id': 3, 'nombre_ciudad': 'barranquilla'}, {'ciudad_id': 4, 'nombre_ciudad': 'new york'}, {'ciudad_id': 5, 'nombre_ciudad': 'madrid'}, {'ciudad_id': 6, 'nombre_ciudad': 'pereira'}, {'ciudad_id': 7, 'nombre_ciudad': 'trujillo'}, {'ciudad_id': 8, 'nombre_ciudad': 'valencia'}  ... displaying 10 of 34 total bound parameter sets ...  {'ciudad_id': 33, 'nombre_ciudad': 'bogota'}, {'ciudad_id': 34, 'nombre_ciudad': None}]]\n(Background on this error at: https://sqlalche.me/e/20/gkpj)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUniqueViolation\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Escritorio/andres_p/Analisis-de-Datos-Python-Power-BI/venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1936\u001b[39m, in \u001b[36mConnection._exec_single_context\u001b[39m\u001b[34m(self, dialect, context, statement, parameters)\u001b[39m\n\u001b[32m   1935\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[32m-> \u001b[39m\u001b[32m1936\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdo_executemany\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1937\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1938\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1939\u001b[39m \u001b[43m            \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1940\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1941\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1942\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m effective_parameters \u001b[38;5;129;01mand\u001b[39;00m context.no_parameters:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Escritorio/andres_p/Analisis-de-Datos-Python-Power-BI/venv/lib/python3.12/site-packages/sqlalchemy/dialects/postgresql/psycopg2.py:800\u001b[39m, in \u001b[36mPGDialect_psycopg2.do_executemany\u001b[39m\u001b[34m(self, cursor, statement, parameters, context)\u001b[39m\n\u001b[32m    799\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m800\u001b[39m     \u001b[43mcursor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecutemany\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mUniqueViolation\u001b[39m: llave duplicada viola restricción de unicidad «ciudad_pkey»\nDETAIL:  Ya existe la llave (ciudad_id)=(1).\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mIntegrityError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     41\u001b[39m         query = text(\u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[33m            INSERT INTO \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolnames\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[33m            VALUES (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplaceholders\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m);\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[33m        \u001b[39m\u001b[33m\"\"\"\u001b[39m)\n\u001b[32m     46\u001b[39m         \u001b[38;5;66;03m# Ejecutar todas las filas del chunk\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m         \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43morient\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrecords\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m was inserted sucess!!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     51\u001b[39m conn.commit()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Escritorio/andres_p/Analisis-de-Datos-Python-Power-BI/venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1419\u001b[39m, in \u001b[36mConnection.execute\u001b[39m\u001b[34m(self, statement, parameters, execution_options)\u001b[39m\n\u001b[32m   1417\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc.ObjectNotExecutableError(statement) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   1418\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1420\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1421\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1422\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mNO_OPTIONS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1423\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Escritorio/andres_p/Analisis-de-Datos-Python-Power-BI/venv/lib/python3.12/site-packages/sqlalchemy/sql/elements.py:526\u001b[39m, in \u001b[36mClauseElement._execute_on_connection\u001b[39m\u001b[34m(self, connection, distilled_params, execution_options)\u001b[39m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[32m    525\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, Executable)\n\u001b[32m--> \u001b[39m\u001b[32m526\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execute_clauseelement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistilled_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_options\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    530\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc.ObjectNotExecutableError(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Escritorio/andres_p/Analisis-de-Datos-Python-Power-BI/venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1641\u001b[39m, in \u001b[36mConnection._execute_clauseelement\u001b[39m\u001b[34m(self, elem, distilled_parameters, execution_options)\u001b[39m\n\u001b[32m   1629\u001b[39m compiled_cache: Optional[CompiledCacheType] = execution_options.get(\n\u001b[32m   1630\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcompiled_cache\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.engine._compiled_cache\n\u001b[32m   1631\u001b[39m )\n\u001b[32m   1633\u001b[39m compiled_sql, extracted_params, cache_hit = elem._compile_w_cache(\n\u001b[32m   1634\u001b[39m     dialect=dialect,\n\u001b[32m   1635\u001b[39m     compiled_cache=compiled_cache,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1639\u001b[39m     linting=\u001b[38;5;28mself\u001b[39m.dialect.compiler_linting | compiler.WARN_LINTING,\n\u001b[32m   1640\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1641\u001b[39m ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1642\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1643\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecution_ctx_cls\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_init_compiled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1644\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompiled_sql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1645\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1646\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1647\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompiled_sql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1648\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1649\u001b[39m \u001b[43m    \u001b[49m\u001b[43melem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1650\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextracted_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1651\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_hit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_hit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1652\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1653\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_events:\n\u001b[32m   1654\u001b[39m     \u001b[38;5;28mself\u001b[39m.dispatch.after_execute(\n\u001b[32m   1655\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1656\u001b[39m         elem,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1660\u001b[39m         ret,\n\u001b[32m   1661\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Escritorio/andres_p/Analisis-de-Datos-Python-Power-BI/venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1846\u001b[39m, in \u001b[36mConnection._execute_context\u001b[39m\u001b[34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[39m\n\u001b[32m   1844\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exec_insertmany_context(dialect, context)\n\u001b[32m   1845\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1846\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_exec_single_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1847\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\n\u001b[32m   1848\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Escritorio/andres_p/Analisis-de-Datos-Python-Power-BI/venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1986\u001b[39m, in \u001b[36mConnection._exec_single_context\u001b[39m\u001b[34m(self, dialect, context, statement, parameters)\u001b[39m\n\u001b[32m   1983\u001b[39m     result = context._setup_result_proxy()\n\u001b[32m   1985\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_dbapi_exception\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1987\u001b[39m \u001b[43m        \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1990\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Escritorio/andres_p/Analisis-de-Datos-Python-Power-BI/venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:2355\u001b[39m, in \u001b[36mConnection._handle_dbapi_exception\u001b[39m\u001b[34m(self, e, statement, parameters, cursor, context, is_sub_exec)\u001b[39m\n\u001b[32m   2353\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m should_wrap:\n\u001b[32m   2354\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m sqlalchemy_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2355\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m sqlalchemy_exception.with_traceback(exc_info[\u001b[32m2\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   2356\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2357\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m exc_info[\u001b[32m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Escritorio/andres_p/Analisis-de-Datos-Python-Power-BI/venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1936\u001b[39m, in \u001b[36mConnection._exec_single_context\u001b[39m\u001b[34m(self, dialect, context, statement, parameters)\u001b[39m\n\u001b[32m   1934\u001b[39m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1935\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[32m-> \u001b[39m\u001b[32m1936\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdo_executemany\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1937\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1938\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1939\u001b[39m \u001b[43m            \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1940\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1941\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1942\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m effective_parameters \u001b[38;5;129;01mand\u001b[39;00m context.no_parameters:\n\u001b[32m   1943\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dialect._has_events:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Escritorio/andres_p/Analisis-de-Datos-Python-Power-BI/venv/lib/python3.12/site-packages/sqlalchemy/dialects/postgresql/psycopg2.py:800\u001b[39m, in \u001b[36mPGDialect_psycopg2.do_executemany\u001b[39m\u001b[34m(self, cursor, statement, parameters, context)\u001b[39m\n\u001b[32m    796\u001b[39m     \u001b[38;5;28mself\u001b[39m._psycopg2_extras.execute_batch(\n\u001b[32m    797\u001b[39m         cursor, statement, parameters, **kwargs\n\u001b[32m    798\u001b[39m     )\n\u001b[32m    799\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m800\u001b[39m     \u001b[43mcursor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecutemany\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mIntegrityError\u001b[39m: (psycopg2.errors.UniqueViolation) llave duplicada viola restricción de unicidad «ciudad_pkey»\nDETAIL:  Ya existe la llave (ciudad_id)=(1).\n\n[SQL: \n                INSERT INTO ciudad (ciudad_id, nombre_ciudad)\n                VALUES (%(ciudad_id)s, %(nombre_ciudad)s);\n            ]\n[parameters: [{'ciudad_id': 1, 'nombre_ciudad': 'santiago'}, {'ciudad_id': 2, 'nombre_ciudad': 'cordoba'}, {'ciudad_id': 3, 'nombre_ciudad': 'barranquilla'}, {'ciudad_id': 4, 'nombre_ciudad': 'new york'}, {'ciudad_id': 5, 'nombre_ciudad': 'madrid'}, {'ciudad_id': 6, 'nombre_ciudad': 'pereira'}, {'ciudad_id': 7, 'nombre_ciudad': 'trujillo'}, {'ciudad_id': 8, 'nombre_ciudad': 'valencia'}  ... displaying 10 of 34 total bound parameter sets ...  {'ciudad_id': 33, 'nombre_ciudad': 'bogota'}, {'ciudad_id': 34, 'nombre_ciudad': None}]]\n(Background on this error at: https://sqlalche.me/e/20/gkpj)"
     ]
    }
   ],
   "source": [
    "#This order respect the cardinality and prevent issues\n",
    "order_tables = [\n",
    "    \"ciudad\",\n",
    "    \"tipo_producto\",\n",
    "    \"producto\",\n",
    "    \"tipo_venta\",\n",
    "    \"tipo_cliente\",\n",
    "    \"factura_ventas\"\n",
    "]\n",
    "\n",
    "SEEDERS_DIR = \"../tables_csv\"\n",
    "\n",
    "CHUNK_SIZE = 100000  \n",
    "\n",
    "with engine.connect() as conn:\n",
    "    \n",
    "    conn.execute(text(\"SET search_path TO riwi_ventas;\"))\n",
    "\n",
    "    for table in order_tables:\n",
    "        csv_path = os.path.join(SEEDERS_DIR, f\"{table}.csv\")\n",
    "\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"doesn't exist: {csv_path}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nInsert into table: {table}\")\n",
    "\n",
    "        # Read by chunk\n",
    "        chunk_iter = pd.read_csv(csv_path, chunksize=CHUNK_SIZE)\n",
    "\n",
    "        for i, chunk_df in enumerate(chunk_iter):\n",
    "            print(f\"  → Chunk {i+1} inserting... ({len(chunk_df)} rows)\")\n",
    "\n",
    "            # Replace NaN per None or Null for SQL\n",
    "            chunk_df = chunk_df.where(pd.notnull(chunk_df), None)\n",
    "\n",
    "            cols = chunk_df.columns.tolist()\n",
    "            colnames = \", \".join(cols)\n",
    "            placeholders = \", \".join([f\":{c}\" for c in cols])\n",
    "\n",
    "            query = text(f\"\"\"\n",
    "                INSERT INTO {table} ({colnames})\n",
    "                VALUES ({placeholders});\n",
    "            \"\"\")\n",
    "\n",
    "            # Ejecutar todas las filas del chunk\n",
    "            conn.execute(query, chunk_df.to_dict(orient=\"records\"))\n",
    "\n",
    "        print(f\"Table {table} was inserted sucess!!\")\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "    conn.close()\n",
    "    \n",
    "print(\"Data inserted sucessfull!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
