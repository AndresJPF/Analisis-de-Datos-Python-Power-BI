{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cef24cd",
   "metadata": {},
   "source": [
    "# Observación, limpieza y carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748bbe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import unicodedata\n",
    "import re\n",
    "import os\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "df = pd.read_csv(\"../ventas.csv\")\n",
    "\n",
    "print(df)\n",
    "# [1.048.575 rows x 11 columns]\n",
    "# 11 Columnas\n",
    "# 1.048.575 Registros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d717d0b",
   "metadata": {},
   "source": [
    "### Oservación\n",
    "* Observamos las caracteris y tipos de datos dentro del Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76d330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df.columns:\n",
    "    print(f\"'{row}' {df[row].unique()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f951351",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f85bebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb998aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b56dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608a278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../ventas.csv\")\n",
    "def limpiar_ciudad(texto):\n",
    "    if pd.isna(texto):\n",
    "        return texto\n",
    "    \n",
    "    # Converte el texto a minúsculas\n",
    "    texto = texto.lower().strip()\n",
    "\n",
    "    # Elimina los acentos\n",
    "    texto = ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', texto)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "    # Elimina caracteres especiales (dejando solo las letras)\n",
    "    texto = re.sub(r'[^a-z\\s]', '', texto)\n",
    "\n",
    "    # Quita los espacios dobles\n",
    "    texto = re.sub(r'\\s+', ' ', texto)\n",
    "\n",
    "    return texto\n",
    "# 1. Normalización de texto\n",
    "cols_texto = [\"Ciudad\", \"Producto\", \"Tipo_Producto\", \"Tipo_de_Venta\", \"Tipo_Cliente\"]\n",
    "for col in cols_texto:\n",
    "    df[col] = df[col].apply(limpiar_ciudad)\n",
    "\n",
    "# 2. Converción a numéricos\n",
    "df[\"Descuento\"] = pd.to_numeric(df[\"Descuento\"], errors=\"coerce\")\n",
    "df[\"Precio_Unitario\"] = pd.to_numeric(df[\"Precio_Unitario\"], errors=\"coerce\")\n",
    "df[\"Cantidad\"] = pd.to_numeric(df[\"Cantidad\"], errors=\"coerce\")\n",
    "df[\"Costo_Envio\"] = pd.to_numeric(df[\"Costo_Envio\"], errors=\"coerce\")\n",
    "df[\"Total_Venta\"] = pd.to_numeric(df[\"Total_Venta\"], errors=\"coerce\")\n",
    "\n",
    "# 3. Rellenar faltantes\n",
    "df[\"Tipo_Cliente\"] = df[\"Tipo_Cliente\"].replace(\"nan\", np.nan)\n",
    "df[\"Tipo_Cliente\"] = df[\"Tipo_Cliente\"].fillna(\"desconocido\")\n",
    "\n",
    "df[\"Descuento\"] = df[\"Descuento\"].fillna(0)\n",
    "df[\"Precio_Unitario\"] = df[\"Precio_Unitario\"].fillna(0)\n",
    "df[\"Cantidad\"] = df[\"Cantidad\"].fillna(0)\n",
    "df[\"Costo_Envio\"] = df[\"Costo_Envio\"].fillna(0)\n",
    "df[\"Total_Venta\"] = df[\"Total_Venta\"].fillna(0)\n",
    "\n",
    "# 4. Converción a fecha\n",
    "df[\"Fecha\"] = pd.to_datetime(df[\"Fecha\"], errors=\"coerce\")\n",
    "\n",
    "\n",
    "# 5. Recalcular el valor de Total_Venta\n",
    "df[\"Total_Venta\"] = (df[\"Cantidad\"] * df[\"Precio_Unitario\"]) - (df[\"Descuento\"] * df[\"Cantidad\"] * df[\"Precio_Unitario\"]) + df[\"Costo_Envio\"]\n",
    "\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "# 6. Guarda los cambios en  un nuevo archivo .csv\n",
    "\n",
    "df.to_csv(\"../ventas_limpias.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e05f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768dc88a",
   "metadata": {},
   "source": [
    "### Carga de Datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f42b307e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql+psycopg2://postgres:Qwe.123*@localhost:5432/Riwi_Sports\n",
      "Connection Succesfull!!\n"
     ]
    }
   ],
   "source": [
    "#Carga el archivo .env\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "#Variables de entorno\n",
    "\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "URL = f\"postgresql+psycopg2://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "engine = create_engine(URL)\n",
    "print(URL)\n",
    "conn = engine.connect()\n",
    "try:\n",
    "    print(\"Connection Succesfull!!\" if conn else \"\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error al conectar la base datos en\", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3c34d9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table: 'Ciudad' doesn't Exist or is Empty.\n",
      "Table: 'Tipo_producto' doesn't Exist or is Empty.\n",
      "Table: 'Producto' doesn't Exist or is Empty.\n",
      "Table: 'Tipo_venta' doesn't Exist or is Empty.\n",
      "Table: 'Tipo_cliente' doesn't Exist or is Empty.\n",
      "Table: 'Factura_ventas' doesn't Exist or is Empty.\n"
     ]
    }
   ],
   "source": [
    "set_schema = conn.execute(text(\"set search_path to riwi_ventas;\"))\n",
    "\n",
    "tables = [\"ciudad\", \"tipo_producto\", \"producto\", \"tipo_venta\", \"tipo_cliente\", \"factura_ventas\"]\n",
    "\n",
    "for t in range(len(tables)):\n",
    "\n",
    "    verify = pd.read_sql((f\"select * from {tables[t]};\"),conn)\n",
    "    if not verify.empty:\n",
    "        print(f\"Talbe: '{tables[t].capitalize()}' Exist and is Filled.\")\n",
    "    if verify.empty:\n",
    "        print(f\"Table: '{tables[t].capitalize()}' doesn't Exist or is Empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a4aab2",
   "metadata": {},
   "source": [
    "### Generate a individual csv per table before to load to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "de607850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV creados correctamente en la carpeta 'tables_csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "cleaned_df = pd.read_csv(\"../ventas_limpias.csv\")\n",
    "\n",
    "#Obtain the unique cities and generate the ID for Cities\n",
    "ciudad_df = (\n",
    "    cleaned_df[\"ciudad\"].drop_duplicates()\n",
    "                .reset_index(drop=True)\n",
    "                .reset_index()\n",
    "                .rename(columns={\"index\": \"ciudad_id\", \"ciudad\": \"nombre_ciudad\"})\n",
    "                \n",
    ")\n",
    "ciudad_df[\"ciudad_id\"] += 1\n",
    "\n",
    "cleaned_df =cleaned_df.merge(ciudad_df, left_on=\"ciudad\", right_on=\"nombre_ciudad\", how=\"left\",suffixes=(\"\", \"_drop\"))\n",
    "\n",
    "\n",
    "# Obtain the unique cities and generate the ID for Products\n",
    "\n",
    "tipo_producto_df = (\n",
    "    cleaned_df[\"tipo_producto\"].drop_duplicates()\n",
    "                       .reset_index(drop=True)\n",
    "                       .reset_index()\n",
    "                       .rename(columns={\"index\": \"tipo_producto_id\"})\n",
    ")\n",
    "tipo_producto_df[\"tipo_producto_id\"] += 1\n",
    "\n",
    "cleaned_df =cleaned_df.merge(tipo_producto_df, on=\"tipo_producto\", how=\"left\",suffixes=(\"\", \"_drop\"))\n",
    "\n",
    "\n",
    "# Obtain the unique cities and generate the ID for type products\n",
    "\n",
    "cleaned_df = cleaned_df.rename(columns={\"producto\":\"nombre_producto\"}) #Rename to prevent conflicts with DB\n",
    "\n",
    "producto_df = (\n",
    "    cleaned_df[[\"nombre_producto\", \"tipo_producto_id\"]]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"producto_id\", \"producto\":\"nombre_producto\"})\n",
    ")\n",
    "producto_df[\"producto_id\"] += 1\n",
    "\n",
    "cleaned_df =cleaned_df.merge(producto_df, on=[\"nombre_producto\", \"tipo_producto_id\"], how=\"left\",suffixes=(\"\", \"_drop\"))\n",
    "\n",
    "# Obtain the unique cities and generate the ID for type sale\n",
    "\n",
    "cleaned_df = cleaned_df.rename(columns={\"tipo_de_venta\":\"tipo_venta\"}) #Rename to prevent conflicts with DB\n",
    "\n",
    "tipo_venta_df = (\n",
    "    cleaned_df[\"tipo_venta\"].drop_duplicates()\n",
    "                       .reset_index(drop=True)\n",
    "                       .reset_index()\n",
    "                       .rename(columns={\"index\": \"tipo_venta_id\"})\n",
    ")\n",
    "tipo_venta_df[\"tipo_venta_id\"] += 1\n",
    "\n",
    "cleaned_df =cleaned_df.merge(tipo_venta_df, on=\"tipo_venta\", how=\"left\",suffixes=(\"\", \"_drop\"))\n",
    "\n",
    "\n",
    "# Obtain the unique cities and generate the ID for type customer\n",
    "tipo_cliente_df = (\n",
    "    cleaned_df[\"tipo_cliente\"].drop_duplicates()\n",
    "                      .reset_index(drop=True)\n",
    "                      .reset_index()\n",
    "                      .rename(columns={\"index\": \"tipo_cliente_id\"})\n",
    ")\n",
    "tipo_cliente_df[\"tipo_cliente_id\"] += 1\n",
    "\n",
    "cleaned_df =cleaned_df.merge(tipo_cliente_df, on=\"tipo_cliente\", how=\"left\",suffixes=(\"\", \"_drop\"))\n",
    "\n",
    "\n",
    "#Create the table for sales\n",
    "factura_df =cleaned_df[[\n",
    "    \"fecha\", \"ciudad_id\", \"producto_id\", \"tipo_venta_id\",\n",
    "    \"tipo_cliente_id\", \"cantidad\", \"precio_unitario\",\n",
    "    \"descuento\", \"costo_envio\", \"total_venta\"\n",
    "]].copy()\n",
    "\n",
    "\n",
    "#export tables to external folder\n",
    "os.makedirs(\"../tables_csv\", exist_ok=True)\n",
    "\n",
    "#Save the files into tables_csv\n",
    "ciudad_df.to_csv(\"../tables_csv/ciudad.csv\", index=False)\n",
    "tipo_producto_df.to_csv(\"../tables_csv/tipo_producto.csv\", index=False)\n",
    "producto_df.to_csv(\"../tables_csv/producto.csv\", index=False)\n",
    "tipo_venta_df.to_csv(\"../tables_csv/tipo_venta.csv\", index=False)\n",
    "tipo_cliente_df.to_csv(\"../tables_csv/tipo_cliente.csv\", index=False)\n",
    "factura_df.to_csv(\"../tables_csv/factura_ventas.csv\", index=False)\n",
    "\n",
    "print(\"CSV creados correctamente en la carpeta 'tables_csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623d0499",
   "metadata": {},
   "source": [
    "### Load data to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "15c2bd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insertando: ciudad\n",
      "Insertando: tipo_producto\n",
      "Insertando: producto\n",
      "Insertando: tipo_venta\n",
      "Insertando: tipo_cliente\n",
      "Insertando: factura_ventas\n",
      "Data inserted sucessfull!!\n"
     ]
    }
   ],
   "source": [
    "#This order respect the cardinality and prevent issues\n",
    "order_tables = [\n",
    "    \"ciudad\",\n",
    "    \"tipo_producto\",\n",
    "    \"producto\",\n",
    "    \"tipo_venta\",\n",
    "    \"tipo_cliente\",\n",
    "    \"factura_ventas\"\n",
    "]\n",
    "\n",
    "SEEDERS_DIR = \"../tables_csv\"\n",
    "\n",
    "\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    \n",
    "    conn.execute(text(\"SET search_path TO riwi_ventas;\"))\n",
    "\n",
    "    for table in order_tables:\n",
    "        csv_path = os.path.join(SEEDERS_DIR, f\"{table}.csv\")\n",
    "\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"No existe: {csv_path}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Insertando: {table}\")\n",
    "\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df = df.where(pd.notnull(df),None)\n",
    "        \n",
    "        cols = df.columns.tolist()\n",
    "        colnames = \", \".join(cols)\n",
    "        placeholders = \", \".join([f\":{c}\" for c in cols])\n",
    "\n",
    "        query = text(f\"INSERT INTO {table} ({colnames}) VALUES ({placeholders})\")\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            conn.execute(query, row.to_dict())\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "print(\"Data inserted sucessfull!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
